{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import qtorch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "hidden_size = 128\n",
    "lr = 0.01 \n",
    "epochs = 50\n",
    "momentum = 0.9\n",
    "rounding = \"stochastic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_name = \"./test_binarizzazione_grafici/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = datasets.MNIST(\"./data\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Images have same size\n",
    "input_size = np.prod(training_data[0][0].shape[1:])\n",
    "output_size = 10\n",
    "\n",
    "# images, labels = training_loader.next()\n",
    "\n",
    "# plt.imshow(images[1][0], cmap=\"gray\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(LinearModule, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(input_size, hidden_size, bias=False, device=device)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size, bias=False, device=device)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size, bias=False, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "model = LinearModule(input_size, output_size, hidden_size)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=momentum, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"./models/mnist_float32_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(model):\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        acc = 0 \n",
    "\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images.reshape(-1, input_size))\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            acc += (predictions == labels).sum().item()\n",
    "            \n",
    "        acc = 100 * acc / len(test_data)            \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAccuracy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModule(input_size, output_size, hidden_size)\n",
    "model.load_state_dict(torch.load(\"./models/mnist_float32_model\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binarize(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.where(torch.ge(x, 0.), 1., 0.)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return torch.nn.functional.hardtanh(grad_output)\n",
    "        \n",
    "binarize = Binarize.apply\n",
    "\n",
    "# Funzione per approssimare alla potenza di 2 più vicina\n",
    "def AP2(x):\n",
    "    return torch.sign(x) * torch.pow(2, torch.round(torch.log2(torch.abs(x))))\n",
    "\n",
    "\n",
    "def batchNorm(x, gamma, beta, inference = False, E = None, Var = None, lastLevel = False):\n",
    "    eps = 1e-5\n",
    "    b_num = E * gamma\n",
    "    b_den = torch.sqrt(Var + eps).pow_(-1)\n",
    "    b = beta - b_num * b_den\n",
    "    w = gamma * torch.sqrt(Var + eps).pow_(-1) \n",
    "    y = AP2(w) * x + b              # Moltiplicazione -> shift register (mul per potenza 2)\n",
    "        \n",
    "    return y\n",
    "    \n",
    "def lossFunction(a, a_star):\n",
    "    # MSE\n",
    "    y = a - a_star\n",
    "    y2 = torch.pow(y, 2)\n",
    "    return torch.mean(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarizedModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(BinarizedModel, self).__init__()\n",
    "        self.total_weights = 0              # Numero di pesi totali nella rete\n",
    "        self.hidden_layer = -1              # Numero di livelli nascosti\n",
    "        self.number_perceptron_layer = []   # Numero di percettroni in ogni livello\n",
    "        self.in_perceptron_layer = []       # Numero di ingressi del percettrone del livello i\n",
    "\n",
    "        for x in model.parameters():\n",
    "            self.in_perceptron_layer.append(x.shape[1])\n",
    "            self.number_perceptron_layer.append(x.shape[0])\n",
    "            self.total_weights += x.shape[0] * x.shape[1]\n",
    "            self.hidden_layer += 1\n",
    "\n",
    "        self.L = self.hidden_layer + 1     # numero di livelli della rete\n",
    "\n",
    "        self.weights = []\n",
    "        for x in model.parameters():\n",
    "            weights = torch.empty(x.shape[1], x.shape[0], device=device)  # Numero di Pesi x Numero di Percettroni\n",
    "            for i in range(x.shape[0]):\n",
    "                for j in range(x.shape[1]):\n",
    "                    weights[j][i] = x[i][j].item()\n",
    "            self.weights.append(weights)\n",
    "\n",
    "        # Paramatri per la backnormalization\n",
    "        self.gamma = torch.empty(self.L, device=device)\n",
    "        self.beta = torch.zeros(self.L, device=device)\n",
    "        nn.init.uniform_(self.gamma)\n",
    "\n",
    "        # Parametri trainable\n",
    "        self.weights = nn.ParameterList(self.weights)       # Pesi della rete\n",
    "        self.beta = nn.Parameter(self.beta)                 # Beta e Gamma per la backnorm\n",
    "        self.gamma = nn.Parameter(self.gamma)\n",
    "\n",
    "        # Parametri non trainable\n",
    "        # Lista delle medie sui batch delle uscite dei percettroni per ogni livello\n",
    "        self.EP = [torch.empty(self.number_perceptron_layer[k], device=device) for k in range(self.L)]\n",
    "        self.EP = nn.ParameterList(self.EP).requires_grad_(False)\n",
    "        # Lista delle varianze delle uscite dei percettroni per ogni livello\n",
    "        self.VarP = [torch.empty(self.number_perceptron_layer[k], device=device) for k in range(self.L)]\n",
    "        self.VarP = nn.ParameterList(self.VarP).requires_grad_(False)\n",
    "                \n",
    "        # Fuzione di attivazione\n",
    "        self.actFun = nn.ReLU()\n",
    "\n",
    "        self.init() \n",
    "    \n",
    "    # Funzione per la inizializzazione dei parametri\n",
    "    # Da invocare prima di iniziare il traning\n",
    "    def init(self):\n",
    "\n",
    "        # Variabili utili per il forward                  \n",
    "        self.W = self.weights\n",
    "        self.Wb = [None] * (self.L)\n",
    "        self.s = [None] * (self.L)\n",
    "        self.a = [None] * (self.L + 1)\n",
    "        self.ab = [None] * (self.L + 1)\n",
    "\n",
    "        # Traccia media popolazione traning\n",
    "        self.EPList = [[] for i in range(self.L)]\n",
    "        self.VarList = [[] for i in range(self.L)]\n",
    "\n",
    "        # inference = false\n",
    "        #   calcolo delle statistiche della popolazione\n",
    "        #   da impostare prima della fase di traning\n",
    "        # inference = true\n",
    "        #   utilizzo delle statistiche della popolazione calcolate\n",
    "        #   da impostare prima della fase di inferenza\n",
    "        self.inference = False \n",
    "\n",
    "\n",
    "    def forward(self, x_vect):\n",
    "\n",
    "        self.a[-1] = x_vect - 0.5    # Normalizzazione dei dati di input [-1; 1]\n",
    "        self.ab[-1] = binarize(self.a[-1])\n",
    "        ab = self.ab[-1]\n",
    "\n",
    "        batch = len(x_vect) > 1\n",
    "        \n",
    "        for k in range(self.L):\n",
    "\n",
    "            # Paper\n",
    "            self.Wb[k] = binarize(self.W[k])    # Binarizzazione dei pesi\n",
    "            self.s[k] = torch.matmul(ab, self.Wb[k])\n",
    "            if not self.inference:\n",
    "                self.a[k] = batchNorm(self.s[k], self.gamma[k], self.beta[k])\n",
    "            else:\n",
    "                self.a[k] = batchNorm(self.s[k], self.gamma[k], self.beta[k], True, self.EP[k], self.VarP[k])\n",
    "            if k < self.L - 1:\n",
    "                self.ab[k] = binarize(self.a[k])\n",
    "            ab = self.ab[k]\n",
    "            # ---------------\n",
    "\n",
    "            # traccia media popolazione traning\n",
    "            if not self.inference:\n",
    "                self.EPList[k].append(torch.mean(self.s[k], dim=0))\n",
    "                self.VarList[k].append(torch.var(self.s[k], dim=0))\n",
    "            # -------------------\n",
    "\n",
    "        return self.a[self.L - 1]\n",
    "\n",
    "\n",
    "    def frozeParameter(self, batch_size):\n",
    "\n",
    "        for k in range(self.L):\n",
    "            eP = torch.stack(self.EPList[k], dim=0).to(device)\n",
    "            varP = torch.stack(self.VarList[k], dim=0).to(device)\n",
    "            self.EP[k] = torch.mean(eP, dim=0)\n",
    "            self.VarP[k] = torch.mean(varP, dim=0).mul_(batch_size / (batch_size - 1))\n",
    "\n",
    "        # Cencellazione dei parametri di traning utilizzati per il calcolo delle statistiche\n",
    "        # della popolazione\n",
    "        self.EPList = [[] for i in range(self.L)]\n",
    "        self.VarList = [[] for i in range(self.L)]\n",
    "\n",
    "bmodel = BinarizedModel(model)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(bmodel.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Binarized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = BinarizedModel(model)\n",
    "bmodel.load_state_dict(torch.load(\"./models/mnist_binarized_model\"))\n",
    "bmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel.inference = True\n",
    "getAccuracy(bmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized Model Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchNormE(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "def  XnorPopCount(x, w):\n",
    "    \n",
    "    res = torch.empty(len(x), len(w), device=device)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        xnor = torch.logical_not(torch.logical_xor(w, x[i])).int()\n",
    "        xnor = 2 * xnor - 1\n",
    "        res[i] = torch.sum(xnor, dim=1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "class BinarizedModelEmbedded():\n",
    "\n",
    "    def __init__(self, bmodel):\n",
    "\n",
    "        self.weights = [None] * len(bmodel.weights)\n",
    "\n",
    "        for k in range(len(bmodel.weights)):\n",
    "            self.weights[k] = binarize(bmodel.weights[k])  # [Livello] [Peso] [Percettrone]\n",
    "            self.weights[k].t_()                           # [Livello] [Percettrone] [Peso]\n",
    "        \n",
    "        self.L = bmodel.L\n",
    "\n",
    "        self.batch_norm_a = [bmodel.gamma[k] / torch.sqrt(bmodel.VarP[k] + 1e-5) for k in range(len(bmodel.gamma))]\n",
    "        self.batch_norm_b = [bmodel.beta[k] - bmodel.gamma[k] * bmodel.EP[k] / torch.sqrt(bmodel.VarP[k] + 1e-5) for k in range(len(bmodel.gamma))]\n",
    "\n",
    "    def forward(self, x_vect):\n",
    "\n",
    "        self.Wb = self.weights\n",
    "        self.s = [None] * (self.L)\n",
    "        self.a = [None] * (self.L + 1)\n",
    "        self.ab = [None] * (self.L + 1)\n",
    "\n",
    "        self.a[-1] = x_vect - 0.5    # Normalizzazione dei dati di input [-1; 1]\n",
    "        self.ab[-1] = binarize(self.a[-1])\n",
    "        ab = self.ab[-1]            # Su una riga c'è l'immagine\n",
    "\n",
    "        batch = len(x_vect) > 1\n",
    "        \n",
    "        for k in range(self.L):\n",
    "\n",
    "            # Paper\n",
    "            self.s[k] = XnorPopCount(ab, self.Wb[k])\n",
    "            self.a[k] = batchNormE(self.s[k], self.batch_norm_a[k], self.batch_norm_b[k])\n",
    "            if k < self.L - 1:\n",
    "                self.ab[k] = binarize(self.a[k])\n",
    "            ab = self.ab[k]\n",
    "\n",
    "        return self.a[self.L - 1]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "bmodelE = BinarizedModelEmbedded(bmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 0, 1], [1, 1, 0]], dtype=int)\n",
    "w = torch.tensor([[1, 0, 0], [1, 0, 1]], dtype=int)\n",
    "print(XnorPopCount(x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAccuracy(bmodelE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a32be92701276002c3a8928f39e8df46bba964be6a742c4fcb624c4dac10f21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
