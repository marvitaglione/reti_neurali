{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./utils/\")\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.utils.prune as prune\n",
    "import utils.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "kernel_size = 3\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "momentum = 0.9\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_transoform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_transoform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "training_data = datasets.CIFAR10(\"./data\", train=True, transform=training_transoform, download=True)\n",
    "test_data = datasets.CIFAR10(\"./data\", train=False, transform=test_transoform, download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Images have same size\n",
    "input_size = training_data[0][0].shape\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "# training_loader = iter(training_loader)\n",
    "# images, labels = training_loader.next()\n",
    "\n",
    "# plt.figure(figsize=(2,2))\n",
    "# plt.imshow(np.transpose(images[1], (1,2,0)))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size, output_size, device):\n",
    "\n",
    "        super(Cifar10Network, self).__init__()\n",
    "\n",
    "        self.l1 = torch.nn.Conv2d(3, 64, kernel_size, bias=False, device=device)\n",
    "        self.b1 = torch.nn.BatchNorm2d(64, device=device)\n",
    "        self.r1 = torch.nn.ReLU()\n",
    "\n",
    "        self.l2 = torch.nn.Conv2d(64, 64, kernel_size, bias=False, device=device)\n",
    "        self.b2 = torch.nn.BatchNorm2d(64, device=device)\n",
    "        self.r2 = torch.nn.ReLU()\n",
    "        self.m2 = torch.nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.l3 = torch.nn.Conv2d(64, 128, kernel_size, bias=False, device=device)\n",
    "        self.b3 = torch.nn.BatchNorm2d(128, device=device)\n",
    "        self.r3 = torch.nn.ReLU()\n",
    "\n",
    "        self.l4 = torch.nn.Conv2d(128, 128, kernel_size, bias=False, device=device)\n",
    "        self.b4 = torch.nn.BatchNorm2d(128, device=device)\n",
    "        self.r4 = torch.nn.ReLU()\n",
    "        self.m4 = torch.nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.l5 = torch.nn.Conv2d(128, 256, kernel_size, bias=False, device=device)\n",
    "        self.b5 = torch.nn.BatchNorm2d(256, device=device)\n",
    "        self.r5 = torch.nn.ReLU()\n",
    "\n",
    "        self.l6 = torch.nn.Conv2d(256, 256, kernel_size, bias=False, device=device)\n",
    "        self.b6 = torch.nn.BatchNorm2d(256, device=device)\n",
    "        self.r6 = torch.nn.ReLU()\n",
    "\n",
    "        self.f7 = torch.nn.Flatten()\n",
    "        self.l7 = torch.nn.Linear(256, 512, bias=False, device=device)\n",
    "        self.b7 = torch.nn.BatchNorm1d(512, device=device)\n",
    "\n",
    "        self.l8 = torch.nn.Linear(512, 512, bias=False, device=device)\n",
    "        self.b8 = torch.nn.BatchNorm1d(512, device=device)\n",
    "\n",
    "        self.l9 = torch.nn.Linear(512, 10, bias=False, device=device)\n",
    "        self.b9 = torch.nn.BatchNorm1d(10, device=device)\n",
    "        \n",
    "        self.softMax = torch.nn.Softmax(dim=0)\n",
    "        self.relu = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        out = self.l1(x)\n",
    "        out = self.b1(out)\n",
    "        out = self.r1(out)\n",
    "\n",
    "        out = self.l2(out) \n",
    "        out = self.b2(out) \n",
    "        out = self.r2(out)\n",
    "        out = self.m2(out) \n",
    "\n",
    "        out = self.l3(out)\n",
    "        out = self.b3(out) \n",
    "        out = self.r3(out)\n",
    "        \n",
    "        out = self.l4(out) \n",
    "        out = self.b4(out) \n",
    "        out = self.r4(out)\n",
    "        out = self.m4(out) \n",
    "\n",
    "        out = self.l5(out) \n",
    "        out = self.b5(out)\n",
    "        out = self.r5(out) \n",
    "\n",
    "        out = self.l6(out) \n",
    "        out = self.b6(out) \n",
    "        out = self.r6(out)\n",
    "\n",
    "        out = self.f7(out)\n",
    "        out = self.l7(out)\n",
    "        out = self.b7(out)\n",
    "\n",
    "        out = self.l8(out)\n",
    "        out = self.b8(out) \n",
    "\n",
    "        out = self.l9(out) \n",
    "        out = self.b9(out) \n",
    "\n",
    "        return out\n",
    "\n",
    "model = Cifar10Network(kernel_size, output_size, device)  \n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # writer = SummaryWriter(\"./test_grafici/Cifar10Network\")\n",
    "# save_path = \"./models/Cifar10Network_model\"\n",
    "\n",
    "# maxAcc = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     loss = None\n",
    "\n",
    "#     for i, (images, labels) in enumerate(training_loader):\n",
    "        \n",
    "#         step = (i+1) + epoch * len(training_loader)\n",
    "\n",
    "#         # Forward phase\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         predictions = model(images)\n",
    "\n",
    "#         labels_pred = torch.zeros(len(labels), output_size, device=device)\n",
    "#         for j in range(len(labels)):\n",
    "#             labels_pred[j][labels[j]] = 1\n",
    "\n",
    "#         loss = loss_function(predictions, labels_pred)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"epoch = {epoch+1}/{epochs}, step = {i}/{len(training_loader)}, loss = {loss}\")\n",
    "\n",
    "\n",
    "#     # Evaluate accuracy on test dat \n",
    "#     accVal = util.getAccuracy(model, test_loader, device=device, dim=2)\n",
    "#     print(f\"accVal = {accVal}%\")\n",
    "\n",
    "#     if accVal > maxAcc:\n",
    "#         maxAcc = accVal\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "\n",
    "#     # Update learning rate\n",
    "#     #for g in optimizer.param_groups:\n",
    "#     #    g['lr'] = lr * 0.95\n",
    "    \n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Write result on tensorboard\n",
    "#     # writer.add_scalar(\"Training loss\", loss, epoch)\n",
    "#     # writer.add_scalar(\"Validation accuracy\", accVal, epoch)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import load_model\n",
    "from cifar10Model import Cifar10Network\n",
    "model = Cifar10Network(kernel_size, 10, device)\n",
    "load_model(model, \"./models/Cifar10Network_model\")\n",
    "# acc = model.getAccuracy(test_loader)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizzazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customeLayer as cl\n",
    "import binarize as bin\n",
    "\n",
    "class Cifar10NetworkBin(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model: Cifar10Network = None, device=\"cpu\"):\n",
    "\n",
    "        super(Cifar10NetworkBin, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.weights = []\n",
    "        self.bnWeights = []\n",
    "        self.bnBias = []\n",
    "\n",
    "        # Parametri del modello da binarizzare\n",
    "        if model is not None:        \n",
    "            g = list(model.parameters())\n",
    "            for i in range(int(len(g) / 3)):\n",
    "                self.weights.append(g[3*i])\n",
    "                self.bnWeights.append(g[3*i + 1])\n",
    "                self.bnBias.append(g[3*i + 2])\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.weights.append(torch.empty(64,3,3,3, device=self.device))\n",
    "            self.weights.append(torch.empty(64,64,3,3, device=self.device))\n",
    "            self.weights.append(torch.empty(128,64,3,3, device=self.device))\n",
    "            self.weights.append(torch.empty(128,128,3,3, device=self.device))\n",
    "            self.weights.append(torch.empty(256,128,3,3, device=self.device))\n",
    "            self.weights.append(torch.empty(256,256,3,3, device=self.device))\n",
    "            self.weights.append(torch.empty(512,256, device=self.device))\n",
    "            self.weights.append(torch.empty(512,512, device=self.device))\n",
    "            self.weights.append(torch.empty(10,512, device=self.device))\n",
    "\n",
    "            self.bnWeights.append(torch.empty(64, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(64, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(128, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(128, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(256, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(256, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(512, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(512, device=self.device))\n",
    "            self.bnWeights.append(torch.empty(10, device=self.device))\n",
    "            \n",
    "            self.bnBias.append(torch.empty(64, device=self.device))\n",
    "            self.bnBias.append(torch.empty(64, device=self.device))\n",
    "            self.bnBias.append(torch.empty(128, device=self.device))\n",
    "            self.bnBias.append(torch.empty(128, device=self.device))\n",
    "            self.bnBias.append(torch.empty(256, device=self.device))\n",
    "            self.bnBias.append(torch.empty(256, device=self.device))\n",
    "            self.bnBias.append(torch.empty(512, device=self.device))\n",
    "            self.bnBias.append(torch.empty(512, device=self.device))\n",
    "            self.bnBias.append(torch.empty(10, device=self.device))\n",
    "\n",
    "\n",
    "        # Layers\n",
    "        # self.bin1 = cl.BinarizeLayer2Level2D(self.weights[0].shape[1], device=self.device)\n",
    "        self.l1 = cl.BinarizeConv2d(self.weights[0].shape[1], self.weights[0].shape[0], kernel_size=3, device=self.device)\n",
    "        # self.l1 = cl.ConvBinLayer(self.weights[0], device=device, binfunction=bin.binarize11)\n",
    "        # self.b1 = cl.BatchNorm2D(self.bnWeights[0], self.bnBias[0], device=self.device)\n",
    "        self.b1 = torch.nn.BatchNorm2d(self.bnWeights[0].shape[0], device=self.device)\n",
    "        self.ht1 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin2 = cl.BinarizeLayer2Level2D(self.weights[1].shape[1], device=self.device)\n",
    "        self.l2 = cl.BinarizeConv2d(self.weights[1].shape[1], self.weights[1].shape[0], kernel_size=3, device=self.device)\n",
    "        # self.l2 = cl.ConvBinLayer(self.weights[1], device=device, binfunction=bin.binarize11)\n",
    "        # self.b2 = cl.BatchNorm2D(self.bnWeights[1], self.bnBias[1], device=self.device)\n",
    "        self.b2 = torch.nn.BatchNorm2d(self.bnWeights[1].shape[0], device=self.device)\n",
    "        self.m2 = torch.nn.MaxPool2d(2,2)\n",
    "        self.ht2 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin3 = cl.BinarizeLayer2Level2D(self.weights[2].shape[1], device=self.device)\n",
    "        self.l3 = cl.BinarizeConv2d(self.weights[2].shape[1], self.weights[2].shape[0], kernel_size=3, device=self.device)\n",
    "        # self.l3 = cl.ConvBinLayer(self.weights[2], device=device, binfunction=bin.binarize11)\n",
    "        # self.b3 = cl.BatchNorm2D(self.bnWeights[2], self.bnBias[2], device=self.device)\n",
    "        self.b3 = torch.nn.BatchNorm2d(self.bnWeights[2].shape[0], device=self.device)\n",
    "        self.ht3 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin4 = cl.BinarizeLayer2Level2D(self.weights[3].shape[1], device=self.device)\n",
    "        self.l4 = cl.BinarizeConv2d(self.weights[3].shape[1], self.weights[3].shape[0], kernel_size=3, device=self.device)\n",
    "        # self.l4 = cl.ConvBinLayer(self.weights[3], device=device, binfunction=bin.binarize11)\n",
    "        # self.b4 = cl.BatchNorm2D(self.bnWeights[3], self.bnBias[3], device=self.device)\n",
    "        self.b4 = torch.nn.BatchNorm2d(self.bnWeights[3].shape[0], device=self.device)\n",
    "        self.m4 = torch.nn.MaxPool2d(2,2)\n",
    "        self.ht4 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin5 = cl.BinarizeLayer2Level2D(self.weights[4].shape[1], device=self.device)\n",
    "        self.l5 = cl.BinarizeConv2d(self.weights[4].shape[1], self.weights[4].shape[0], kernel_size=3, device=self.device)\n",
    "        # self.l5 = cl.ConvBinLayer(self.weights[4], device=device, binfunction=bin.binarize11)\n",
    "        # self.b5 = cl.BatchNorm2D(self.bnWeights[4], self.bnBias[4], device=self.device)\n",
    "        self.b5 = torch.nn.BatchNorm2d(self.bnWeights[4].shape[0], device=self.device)\n",
    "        self.ht5 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin6 = cl.BinarizeLayer2Level2D(self.weights[5].shape[1], device=self.device)\n",
    "        self.l6 = cl.BinarizeConv2d(self.weights[5].shape[1], self.weights[5].shape[0], kernel_size=3, device=self.device)\n",
    "        # self.l6 = cl.ConvBinLayer(self.weights[5], device=device, binfunction=bin.binarize11)\n",
    "        # self.b6 = cl.BatchNorm2D(self.bnWeights[5], self.bnBias[5], device=self.device)\n",
    "        self.b6 = torch.nn.BatchNorm2d(self.bnWeights[5].shape[0], device=self.device)\n",
    "        self.ht6 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        self.f7 = torch.nn.Flatten()\n",
    "        # self.bin7 = cl.BinarizeLayer2Level(self.weights[6].shape[1], device=self.device)\n",
    "        self.l7 = cl.BinarizeLinear(self.weights[6].shape[1], self.weights[6].shape[0], device=self.device)\n",
    "        # self.l7 = cl.LinearBin(self.weights[6], device=device, binFunction=bin.binarize11)\n",
    "        # self.b7 = cl.BatchNorm1D(512, self.bnWeights[6], self.bnBias[6], device=self.device)\n",
    "        self.b7 = torch.nn.BatchNorm1d(self.bnWeights[6].shape[0], device=self.device)\n",
    "        self.ht7 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin8 = cl.BinarizeLayer2Level(self.weights[7].shape[1], device=self.device)\n",
    "        self.l8 = cl.BinarizeLinear(self.weights[7].shape[1], self.weights[7].shape[0], device=self.device)\n",
    "        # self.l8 = cl.LinearBin(self.weights[7], device=device, binFunction=bin.binarize11)\n",
    "        # self.b8 = cl.BatchNorm1D(512, self.bnWeights[7], self.bnBias[7], device=self.device)\n",
    "        self.b8 = torch.nn.BatchNorm1d(self.bnWeights[7].shape[0], device=self.device)\n",
    "        self.ht8 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        # self.bin9 = cl.BinarizeLayer2Level(self.weights[8].shape[1], device=self.device)\n",
    "        self.l9 = cl.BinarizeLinear(self.weights[8].shape[1], self.weights[8].shape[0], device=self.device)\n",
    "        # self.l9 = cl.LinearBin(self.weights[8], device=device, binFunction=bin.binarize11)\n",
    "        # self.b9 = cl.BatchNorm1D(10, self.bnWeights[8], self.bnBias[8], device=self.device)\n",
    "        self.b9 = torch.nn.BatchNorm1d(self.bnWeights[8].shape[0], device=self.device)\n",
    "        self.ht9 = torch.nn.Hardtanh(inplace=True)\n",
    "\n",
    "        self.softMax = torch.nn.Softmax(dim=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log = torch.nn.LogSoftmax()\n",
    "\n",
    "    \n",
    "    def setInference(self, inference: bool):\n",
    "        return\n",
    "\n",
    "        self.b1.inference = inference\n",
    "        self.b2.inference = inference\n",
    "        self.b3.inference = inference\n",
    "        self.b4.inference = inference\n",
    "        self.b5.inference = inference\n",
    "        self.b6.inference = inference\n",
    "        self.b7.inference = inference\n",
    "        self.b8.inference = inference\n",
    "        self.b9.inference = inference\n",
    "\n",
    "    def frozeParameter(self, batch_size):\n",
    "        return \n",
    "        \n",
    "        self.b1.frozeParameters(batch_size)\n",
    "        self.b2.frozeParameters(batch_size)\n",
    "        self.b3.frozeParameters(batch_size)\n",
    "        self.b4.frozeParameters(batch_size)\n",
    "        self.b5.frozeParameters(batch_size)\n",
    "        self.b6.frozeParameters(batch_size)\n",
    "        self.b7.frozeParameters(batch_size)\n",
    "        self.b8.frozeParameters(batch_size)\n",
    "        self.b9.frozeParameters(batch_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Binarizzazione dell'ingresso\n",
    "        # out = self.bin1(x)\n",
    "        out = self.l1(x)\n",
    "        out = self.b1(out)\n",
    "        out = self.ht1(out)\n",
    "\n",
    "        # out = self.bin2(out)\n",
    "        out = self.l2(out) \n",
    "        out = self.b2(out)\n",
    "        out = self.m2(out)\n",
    "        out = self.ht2(out)\n",
    "\n",
    "        # out = self.bin3(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.ht3(out)\n",
    "        \n",
    "        # out = self.bin4(out)\n",
    "        out = self.l4(out) \n",
    "        out = self.b4(out)\n",
    "        out = self.m4(out)\n",
    "        out = self.ht4(out)\n",
    "\n",
    "        # out = self.bin5(out)\n",
    "        out = self.l5(out) \n",
    "        out = self.b5(out)\n",
    "        out = self.ht5(out)\n",
    "\n",
    "        # out = self.bin6(out)\n",
    "        out = self.l6(out) \n",
    "        out = self.b6(out)\n",
    "        out = self.ht6(out)\n",
    "\n",
    "        out = self.f7(out)\n",
    "        # out = self.bin7(out)\n",
    "        out = self.l7(out)\n",
    "        out = self.b7(out)\n",
    "        out = self.ht7(out)\n",
    "\n",
    "        # out = self.bin8(out)\n",
    "        out = self.l8(out)\n",
    "        out = self.b8(out)\n",
    "        out = self.ht8(out)\n",
    "    \n",
    "        # out = self.bin9(out)\n",
    "        out = self.l9(out)\n",
    "        out = self.b9(out)\n",
    "        # out = self.ht9(out)\n",
    "\n",
    "        return self.log(out)\n",
    "\n",
    "    def regularization(self):\n",
    "        r = 0\n",
    "        for g in self.named_parameters():\n",
    "            if g[0].find(\"weights\") != -1:\n",
    "                r += torch.sum(torch.pow(1 - torch.abs(g[1].data),2))\n",
    "        return r\n",
    "        \n",
    "\n",
    "    def trainModel(self, training_loader, test_loader, epochs=100, lr=0.01, writer = None, PATH = None):\n",
    "\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 200)\n",
    "    \n",
    "        maxAcc = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            total_loss = 0\n",
    "            loss = None\n",
    "            self.setInference(False)\n",
    "\n",
    "            for i, (images, labels) in enumerate(training_loader):\n",
    "                \n",
    "                step = (i+1) + epoch * len(training_loader)\n",
    "\n",
    "                # Forward phase\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predictions = self(images)\n",
    "\n",
    "                # labels_pred = torch.zeros(len(labels), 10, device=self.device)\n",
    "                # for j in range(len(labels)):\n",
    "                #     labels_pred[j][labels[j]] = 1\n",
    "\n",
    "                loss = loss_function(predictions, labels)\n",
    "                total_loss += float(loss)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"epoch = {epoch+1}/{epochs}, step = {i}/{len(training_loader)}, loss = {loss}\")\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.frozeParameter(len(images))\n",
    "                self.setInference(True)\n",
    "                accVal = util.getAccuracy(self, test_loader, device=self.device, dim=2)\n",
    "                print(f\"accVal = {accVal}%\")\n",
    "\n",
    "            # Save the model if it is the best so far \n",
    "            if PATH != None and accVal > maxAcc:\n",
    "                maxAcc = accVal\n",
    "                torch.save(self.state_dict(), PATH)\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Write result on tensorboard\n",
    "            if writer != None:\n",
    "                writer.add_scalar(\"Training loss\", total_loss / len(training_loader), epoch)\n",
    "                writer.add_scalar(\"Validation accuracy\", accVal, epoch) \n",
    "            # -----------------------------------------------------\n",
    "\n",
    "\n",
    "bmodel = Cifar10NetworkBin(model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training del modello binarizzato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mario & Agostino\\AppData\\Local\\Temp\\ipykernel_16008\\1991592359.py:206: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.log(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1/100, step = 0/500, loss = 2.4058034420013428\n",
      "epoch = 1/100, step = 100/500, loss = 2.3098220825195312\n",
      "epoch = 1/100, step = 200/500, loss = 2.1831789016723633\n",
      "epoch = 1/100, step = 300/500, loss = 2.2150771617889404\n",
      "epoch = 1/100, step = 400/500, loss = 2.2769320011138916\n",
      "accVal = 16.03%\n",
      "epoch = 2/100, step = 0/500, loss = 2.236830472946167\n",
      "epoch = 2/100, step = 100/500, loss = 2.208827018737793\n",
      "epoch = 2/100, step = 200/500, loss = 2.299254894256592\n",
      "epoch = 2/100, step = 300/500, loss = 2.3299472332000732\n",
      "epoch = 2/100, step = 400/500, loss = 2.231877326965332\n",
      "accVal = 15.18%\n",
      "epoch = 3/100, step = 0/500, loss = 2.2881555557250977\n",
      "epoch = 3/100, step = 100/500, loss = 2.197570562362671\n",
      "epoch = 3/100, step = 200/500, loss = 2.2876925468444824\n",
      "epoch = 3/100, step = 300/500, loss = 2.150352716445923\n",
      "epoch = 3/100, step = 400/500, loss = 2.2343738079071045\n",
      "accVal = 16.12%\n",
      "epoch = 4/100, step = 0/500, loss = 2.2825205326080322\n",
      "epoch = 4/100, step = 100/500, loss = 2.2742772102355957\n",
      "epoch = 4/100, step = 200/500, loss = 2.2770955562591553\n",
      "epoch = 4/100, step = 300/500, loss = 2.329456090927124\n",
      "epoch = 4/100, step = 400/500, loss = 2.1882619857788086\n",
      "accVal = 15.71%\n",
      "epoch = 5/100, step = 0/500, loss = 2.3084402084350586\n",
      "epoch = 5/100, step = 100/500, loss = 2.2603044509887695\n",
      "epoch = 5/100, step = 200/500, loss = 2.2806403636932373\n",
      "epoch = 5/100, step = 300/500, loss = 2.2538535594940186\n",
      "epoch = 5/100, step = 400/500, loss = 2.2278785705566406\n",
      "accVal = 13.45%\n",
      "epoch = 6/100, step = 0/500, loss = 2.2641632556915283\n",
      "epoch = 6/100, step = 100/500, loss = 2.3034820556640625\n",
      "epoch = 6/100, step = 200/500, loss = 2.296199083328247\n",
      "epoch = 6/100, step = 300/500, loss = 2.2605109214782715\n",
      "epoch = 6/100, step = 400/500, loss = 2.279221296310425\n",
      "accVal = 13.3%\n",
      "epoch = 7/100, step = 0/500, loss = 2.3181583881378174\n",
      "epoch = 7/100, step = 100/500, loss = 2.2068679332733154\n",
      "epoch = 7/100, step = 200/500, loss = 2.2687153816223145\n",
      "epoch = 7/100, step = 300/500, loss = 2.2940261363983154\n",
      "epoch = 7/100, step = 400/500, loss = 2.193976402282715\n",
      "accVal = 14.97%\n",
      "epoch = 8/100, step = 0/500, loss = 2.252995252609253\n",
      "epoch = 8/100, step = 100/500, loss = 2.2574191093444824\n",
      "epoch = 8/100, step = 200/500, loss = 2.1445484161376953\n",
      "epoch = 8/100, step = 300/500, loss = 2.2187652587890625\n",
      "epoch = 8/100, step = 400/500, loss = 2.207885265350342\n",
      "accVal = 15.78%\n",
      "epoch = 9/100, step = 0/500, loss = 2.2638847827911377\n",
      "epoch = 9/100, step = 100/500, loss = 2.3055193424224854\n",
      "epoch = 9/100, step = 200/500, loss = 2.244400978088379\n",
      "epoch = 9/100, step = 300/500, loss = 2.2586333751678467\n",
      "epoch = 9/100, step = 400/500, loss = 2.248037815093994\n",
      "accVal = 15.59%\n",
      "epoch = 10/100, step = 0/500, loss = 2.1857995986938477\n",
      "epoch = 10/100, step = 100/500, loss = 2.2606592178344727\n",
      "epoch = 10/100, step = 200/500, loss = 2.2062087059020996\n",
      "epoch = 10/100, step = 300/500, loss = 2.1397924423217773\n",
      "epoch = 10/100, step = 400/500, loss = 2.320974349975586\n",
      "accVal = 15.94%\n",
      "epoch = 11/100, step = 0/500, loss = 2.196303606033325\n",
      "epoch = 11/100, step = 100/500, loss = 2.1861374378204346\n",
      "epoch = 11/100, step = 200/500, loss = 2.2714858055114746\n",
      "epoch = 11/100, step = 300/500, loss = 2.3010141849517822\n",
      "epoch = 11/100, step = 400/500, loss = 2.278115749359131\n",
      "accVal = 15.52%\n",
      "epoch = 12/100, step = 0/500, loss = 2.230919599533081\n",
      "epoch = 12/100, step = 100/500, loss = 2.264739513397217\n",
      "epoch = 12/100, step = 200/500, loss = 2.2753021717071533\n",
      "epoch = 12/100, step = 300/500, loss = 2.2419872283935547\n",
      "epoch = 12/100, step = 400/500, loss = 2.2030136585235596\n",
      "accVal = 15.72%\n",
      "epoch = 13/100, step = 0/500, loss = 2.1777498722076416\n",
      "epoch = 13/100, step = 100/500, loss = 2.217564821243286\n",
      "epoch = 13/100, step = 200/500, loss = 2.2068631649017334\n",
      "epoch = 13/100, step = 300/500, loss = 2.2113711833953857\n",
      "epoch = 13/100, step = 400/500, loss = 2.286630868911743\n",
      "accVal = 15.07%\n",
      "epoch = 14/100, step = 0/500, loss = 2.268829107284546\n",
      "epoch = 14/100, step = 100/500, loss = 2.1811249256134033\n",
      "epoch = 14/100, step = 200/500, loss = 2.244148015975952\n",
      "epoch = 14/100, step = 300/500, loss = 2.254763126373291\n",
      "epoch = 14/100, step = 400/500, loss = 2.2324132919311523\n",
      "accVal = 14.05%\n",
      "epoch = 15/100, step = 0/500, loss = 2.1896960735321045\n",
      "epoch = 15/100, step = 100/500, loss = 2.1404526233673096\n",
      "epoch = 15/100, step = 200/500, loss = 2.328666925430298\n",
      "epoch = 15/100, step = 300/500, loss = 2.2299115657806396\n",
      "epoch = 15/100, step = 400/500, loss = 2.2407186031341553\n",
      "accVal = 14.32%\n",
      "epoch = 16/100, step = 0/500, loss = 2.2701146602630615\n",
      "epoch = 16/100, step = 100/500, loss = 2.2384097576141357\n",
      "epoch = 16/100, step = 200/500, loss = 2.2564830780029297\n",
      "epoch = 16/100, step = 300/500, loss = 2.234661340713501\n",
      "epoch = 16/100, step = 400/500, loss = 2.219787359237671\n",
      "accVal = 14.53%\n",
      "epoch = 17/100, step = 0/500, loss = 2.242215633392334\n",
      "epoch = 17/100, step = 100/500, loss = 2.266436815261841\n",
      "epoch = 17/100, step = 200/500, loss = 2.209332227706909\n",
      "epoch = 17/100, step = 300/500, loss = 2.2882907390594482\n",
      "epoch = 17/100, step = 400/500, loss = 2.1465253829956055\n",
      "accVal = 15.29%\n",
      "epoch = 18/100, step = 0/500, loss = 2.188173770904541\n",
      "epoch = 18/100, step = 100/500, loss = 2.237955331802368\n",
      "epoch = 18/100, step = 200/500, loss = 2.2570273876190186\n",
      "epoch = 18/100, step = 300/500, loss = 2.255617618560791\n",
      "epoch = 18/100, step = 400/500, loss = 2.25284743309021\n",
      "accVal = 14.45%\n",
      "epoch = 19/100, step = 0/500, loss = 2.2360610961914062\n",
      "epoch = 19/100, step = 100/500, loss = 2.254385232925415\n",
      "epoch = 19/100, step = 200/500, loss = 2.341047763824463\n",
      "epoch = 19/100, step = 300/500, loss = 2.2867929935455322\n",
      "epoch = 19/100, step = 400/500, loss = 2.224172353744507\n",
      "accVal = 15.39%\n",
      "epoch = 20/100, step = 0/500, loss = 2.2878987789154053\n",
      "epoch = 20/100, step = 100/500, loss = 2.1862549781799316\n",
      "epoch = 20/100, step = 200/500, loss = 2.254678249359131\n",
      "epoch = 20/100, step = 300/500, loss = 2.3344099521636963\n",
      "epoch = 20/100, step = 400/500, loss = 2.2401459217071533\n",
      "accVal = 12.83%\n",
      "epoch = 21/100, step = 0/500, loss = 2.3234784603118896\n",
      "epoch = 21/100, step = 100/500, loss = 2.2364814281463623\n",
      "epoch = 21/100, step = 200/500, loss = 2.267639398574829\n",
      "epoch = 21/100, step = 300/500, loss = 2.2561445236206055\n",
      "epoch = 21/100, step = 400/500, loss = 2.269171714782715\n",
      "accVal = 14.59%\n",
      "epoch = 22/100, step = 0/500, loss = 2.2791426181793213\n",
      "epoch = 22/100, step = 100/500, loss = 2.286336898803711\n",
      "epoch = 22/100, step = 200/500, loss = 2.2671375274658203\n",
      "epoch = 22/100, step = 300/500, loss = 2.2471208572387695\n",
      "epoch = 22/100, step = 400/500, loss = 2.2750964164733887\n",
      "accVal = 14.55%\n",
      "epoch = 23/100, step = 0/500, loss = 2.3211307525634766\n",
      "epoch = 23/100, step = 100/500, loss = 2.2394444942474365\n",
      "epoch = 23/100, step = 200/500, loss = 2.2220637798309326\n",
      "epoch = 23/100, step = 300/500, loss = 2.2580738067626953\n",
      "epoch = 23/100, step = 400/500, loss = 2.2237837314605713\n",
      "accVal = 13.6%\n",
      "epoch = 24/100, step = 0/500, loss = 2.300352096557617\n",
      "epoch = 24/100, step = 100/500, loss = 2.277944326400757\n",
      "epoch = 24/100, step = 200/500, loss = 2.225454330444336\n",
      "epoch = 24/100, step = 300/500, loss = 2.2977828979492188\n",
      "epoch = 24/100, step = 400/500, loss = 2.263026237487793\n",
      "accVal = 14.91%\n",
      "epoch = 25/100, step = 0/500, loss = 2.243745803833008\n",
      "epoch = 25/100, step = 100/500, loss = 2.204052686691284\n",
      "epoch = 25/100, step = 200/500, loss = 2.2499840259552\n",
      "epoch = 25/100, step = 300/500, loss = 2.2586185932159424\n",
      "epoch = 25/100, step = 400/500, loss = 2.2221591472625732\n",
      "accVal = 14.87%\n",
      "epoch = 26/100, step = 0/500, loss = 2.1939399242401123\n",
      "epoch = 26/100, step = 100/500, loss = 2.2334847450256348\n",
      "epoch = 26/100, step = 200/500, loss = 2.2535815238952637\n",
      "epoch = 26/100, step = 300/500, loss = 2.202158212661743\n",
      "epoch = 26/100, step = 400/500, loss = 2.185563087463379\n",
      "accVal = 14.68%\n",
      "epoch = 27/100, step = 0/500, loss = 2.252943992614746\n",
      "epoch = 27/100, step = 100/500, loss = 2.191084861755371\n",
      "epoch = 27/100, step = 200/500, loss = 2.286806583404541\n",
      "epoch = 27/100, step = 300/500, loss = 2.2310361862182617\n",
      "epoch = 27/100, step = 400/500, loss = 2.2223968505859375\n",
      "accVal = 14.78%\n",
      "epoch = 28/100, step = 0/500, loss = 2.249918222427368\n",
      "epoch = 28/100, step = 100/500, loss = 2.267303705215454\n",
      "epoch = 28/100, step = 200/500, loss = 2.2185556888580322\n",
      "epoch = 28/100, step = 300/500, loss = 2.231177806854248\n",
      "epoch = 28/100, step = 400/500, loss = 2.243974208831787\n",
      "accVal = 14.98%\n",
      "epoch = 29/100, step = 0/500, loss = 2.2732179164886475\n",
      "epoch = 29/100, step = 100/500, loss = 2.2166366577148438\n",
      "epoch = 29/100, step = 200/500, loss = 2.30180287361145\n",
      "epoch = 29/100, step = 300/500, loss = 2.216447591781616\n",
      "epoch = 29/100, step = 400/500, loss = 2.204019069671631\n",
      "accVal = 15.17%\n",
      "epoch = 30/100, step = 0/500, loss = 2.2158052921295166\n",
      "epoch = 30/100, step = 100/500, loss = 2.228172540664673\n",
      "epoch = 30/100, step = 200/500, loss = 2.2808003425598145\n",
      "epoch = 30/100, step = 300/500, loss = 2.28525447845459\n",
      "epoch = 30/100, step = 400/500, loss = 2.2044692039489746\n",
      "accVal = 14.91%\n",
      "epoch = 31/100, step = 0/500, loss = 2.2115705013275146\n",
      "epoch = 31/100, step = 100/500, loss = 2.2262392044067383\n",
      "epoch = 31/100, step = 200/500, loss = 2.249826431274414\n",
      "epoch = 31/100, step = 300/500, loss = 2.223703145980835\n",
      "epoch = 31/100, step = 400/500, loss = 2.347705364227295\n",
      "accVal = 14.17%\n",
      "epoch = 32/100, step = 0/500, loss = 2.291646957397461\n",
      "epoch = 32/100, step = 100/500, loss = 2.2274508476257324\n",
      "epoch = 32/100, step = 200/500, loss = 2.2335338592529297\n",
      "epoch = 32/100, step = 300/500, loss = 2.2269082069396973\n",
      "epoch = 32/100, step = 400/500, loss = 2.246990442276001\n",
      "accVal = 15.57%\n",
      "epoch = 33/100, step = 0/500, loss = 2.279327869415283\n",
      "epoch = 33/100, step = 100/500, loss = 2.195258140563965\n",
      "epoch = 33/100, step = 200/500, loss = 2.25510311126709\n",
      "epoch = 33/100, step = 300/500, loss = 2.25483775138855\n",
      "epoch = 33/100, step = 400/500, loss = 2.2595882415771484\n",
      "accVal = 13.41%\n",
      "epoch = 34/100, step = 0/500, loss = 2.2947463989257812\n",
      "epoch = 34/100, step = 100/500, loss = 2.2699825763702393\n",
      "epoch = 34/100, step = 200/500, loss = 2.215639114379883\n",
      "epoch = 34/100, step = 300/500, loss = 2.333207130432129\n",
      "epoch = 34/100, step = 400/500, loss = 2.338421583175659\n",
      "accVal = 14.76%\n",
      "epoch = 35/100, step = 0/500, loss = 2.2917330265045166\n",
      "epoch = 35/100, step = 100/500, loss = 2.2738475799560547\n",
      "epoch = 35/100, step = 200/500, loss = 2.270033359527588\n",
      "epoch = 35/100, step = 300/500, loss = 2.3011956214904785\n",
      "epoch = 35/100, step = 400/500, loss = 2.34310245513916\n",
      "accVal = 13.62%\n",
      "epoch = 36/100, step = 0/500, loss = 2.21235728263855\n",
      "epoch = 36/100, step = 100/500, loss = 2.196420669555664\n",
      "epoch = 36/100, step = 200/500, loss = 2.2212557792663574\n",
      "epoch = 36/100, step = 300/500, loss = 2.277070999145508\n",
      "epoch = 36/100, step = 400/500, loss = 2.2414145469665527\n",
      "accVal = 15.15%\n",
      "epoch = 37/100, step = 0/500, loss = 2.2849528789520264\n",
      "epoch = 37/100, step = 100/500, loss = 2.2103846073150635\n",
      "epoch = 37/100, step = 200/500, loss = 2.266220808029175\n",
      "epoch = 37/100, step = 300/500, loss = 2.2003254890441895\n",
      "epoch = 37/100, step = 400/500, loss = 2.249016761779785\n",
      "accVal = 14.98%\n",
      "epoch = 38/100, step = 0/500, loss = 2.2042925357818604\n",
      "epoch = 38/100, step = 100/500, loss = 2.230121612548828\n",
      "epoch = 38/100, step = 200/500, loss = 2.2335259914398193\n",
      "epoch = 38/100, step = 300/500, loss = 2.2866950035095215\n",
      "epoch = 38/100, step = 400/500, loss = 2.237004518508911\n",
      "accVal = 15.5%\n",
      "epoch = 39/100, step = 0/500, loss = 2.2662525177001953\n",
      "epoch = 39/100, step = 100/500, loss = 2.225520372390747\n",
      "epoch = 39/100, step = 200/500, loss = 2.2504494190216064\n",
      "epoch = 39/100, step = 300/500, loss = 2.2429699897766113\n",
      "epoch = 39/100, step = 400/500, loss = 2.213967800140381\n",
      "accVal = 14.72%\n",
      "epoch = 40/100, step = 0/500, loss = 2.260204792022705\n",
      "epoch = 40/100, step = 100/500, loss = 2.2403037548065186\n",
      "epoch = 40/100, step = 200/500, loss = 2.244204044342041\n",
      "epoch = 40/100, step = 300/500, loss = 2.23091197013855\n",
      "epoch = 40/100, step = 400/500, loss = 2.223073720932007\n",
      "accVal = 14.79%\n",
      "epoch = 41/100, step = 0/500, loss = 2.22587251663208\n",
      "epoch = 41/100, step = 100/500, loss = 2.2334189414978027\n",
      "epoch = 41/100, step = 200/500, loss = 2.2368881702423096\n",
      "epoch = 41/100, step = 300/500, loss = 2.216493606567383\n",
      "epoch = 41/100, step = 400/500, loss = 2.3268070220947266\n",
      "accVal = 15.06%\n",
      "epoch = 42/100, step = 0/500, loss = 2.215632915496826\n",
      "epoch = 42/100, step = 100/500, loss = 2.3112897872924805\n",
      "epoch = 42/100, step = 200/500, loss = 2.2734737396240234\n",
      "epoch = 42/100, step = 300/500, loss = 2.282822370529175\n",
      "epoch = 42/100, step = 400/500, loss = 2.248342514038086\n",
      "accVal = 13.1%\n",
      "epoch = 43/100, step = 0/500, loss = 2.235805034637451\n",
      "epoch = 43/100, step = 100/500, loss = 2.2280266284942627\n",
      "epoch = 43/100, step = 200/500, loss = 2.268127202987671\n",
      "epoch = 43/100, step = 300/500, loss = 2.277698278427124\n",
      "epoch = 43/100, step = 400/500, loss = 2.2827162742614746\n",
      "accVal = 14.25%\n",
      "epoch = 44/100, step = 0/500, loss = 2.2258362770080566\n",
      "epoch = 44/100, step = 100/500, loss = 2.26208233833313\n",
      "epoch = 44/100, step = 200/500, loss = 2.245140790939331\n",
      "epoch = 44/100, step = 300/500, loss = 2.2031354904174805\n",
      "epoch = 44/100, step = 400/500, loss = 2.303277015686035\n",
      "accVal = 14.13%\n",
      "epoch = 45/100, step = 0/500, loss = 2.269463539123535\n",
      "epoch = 45/100, step = 100/500, loss = 2.223313331604004\n",
      "epoch = 45/100, step = 200/500, loss = 2.266361951828003\n",
      "epoch = 45/100, step = 300/500, loss = 2.2677788734436035\n",
      "epoch = 45/100, step = 400/500, loss = 2.251187324523926\n",
      "accVal = 14.47%\n",
      "epoch = 46/100, step = 0/500, loss = 2.305846929550171\n",
      "epoch = 46/100, step = 100/500, loss = 2.2681291103363037\n",
      "epoch = 46/100, step = 200/500, loss = 2.2814455032348633\n",
      "epoch = 46/100, step = 300/500, loss = 2.2770354747772217\n",
      "epoch = 46/100, step = 400/500, loss = 2.2464029788970947\n",
      "accVal = 13.92%\n",
      "epoch = 47/100, step = 0/500, loss = 2.253890037536621\n",
      "epoch = 47/100, step = 100/500, loss = 2.2414515018463135\n",
      "epoch = 47/100, step = 200/500, loss = 2.2784903049468994\n",
      "epoch = 47/100, step = 300/500, loss = 2.247593402862549\n",
      "epoch = 47/100, step = 400/500, loss = 2.312678337097168\n",
      "accVal = 13.16%\n",
      "epoch = 48/100, step = 0/500, loss = 2.2189218997955322\n",
      "epoch = 48/100, step = 100/500, loss = 2.261914014816284\n",
      "epoch = 48/100, step = 200/500, loss = 2.3032784461975098\n",
      "epoch = 48/100, step = 300/500, loss = 2.2618439197540283\n",
      "epoch = 48/100, step = 400/500, loss = 2.291257381439209\n",
      "accVal = 13.57%\n",
      "epoch = 49/100, step = 0/500, loss = 2.2607641220092773\n",
      "epoch = 49/100, step = 100/500, loss = 2.2685253620147705\n",
      "epoch = 49/100, step = 200/500, loss = 2.264982223510742\n",
      "epoch = 49/100, step = 300/500, loss = 2.3094191551208496\n",
      "epoch = 49/100, step = 400/500, loss = 2.2388739585876465\n",
      "accVal = 13.0%\n",
      "epoch = 50/100, step = 0/500, loss = 2.3211305141448975\n",
      "epoch = 50/100, step = 100/500, loss = 2.2367963790893555\n",
      "epoch = 50/100, step = 200/500, loss = 2.328683853149414\n",
      "epoch = 50/100, step = 300/500, loss = 2.219217300415039\n",
      "epoch = 50/100, step = 400/500, loss = 2.237443447113037\n",
      "accVal = 14.02%\n",
      "epoch = 51/100, step = 0/500, loss = 2.213988780975342\n",
      "epoch = 51/100, step = 100/500, loss = 2.207221031188965\n",
      "epoch = 51/100, step = 200/500, loss = 2.2676126956939697\n",
      "epoch = 51/100, step = 300/500, loss = 2.270012617111206\n",
      "epoch = 51/100, step = 400/500, loss = 2.306924343109131\n",
      "accVal = 12.82%\n",
      "epoch = 52/100, step = 0/500, loss = 2.326653480529785\n",
      "epoch = 52/100, step = 100/500, loss = 2.2693891525268555\n",
      "epoch = 52/100, step = 200/500, loss = 2.302035093307495\n",
      "epoch = 52/100, step = 300/500, loss = 2.2668397426605225\n",
      "epoch = 52/100, step = 400/500, loss = 2.285902976989746\n",
      "accVal = 12.55%\n",
      "epoch = 53/100, step = 0/500, loss = 2.2939560413360596\n",
      "epoch = 53/100, step = 100/500, loss = 2.2855324745178223\n",
      "epoch = 53/100, step = 200/500, loss = 2.2603533267974854\n",
      "epoch = 53/100, step = 300/500, loss = 2.2661731243133545\n",
      "epoch = 53/100, step = 400/500, loss = 2.3324830532073975\n",
      "accVal = 14.07%\n",
      "epoch = 54/100, step = 0/500, loss = 2.251596689224243\n",
      "epoch = 54/100, step = 100/500, loss = 2.2731664180755615\n",
      "epoch = 54/100, step = 200/500, loss = 2.2552242279052734\n",
      "epoch = 54/100, step = 300/500, loss = 2.289043664932251\n",
      "epoch = 54/100, step = 400/500, loss = 2.2902650833129883\n",
      "accVal = 13.53%\n",
      "epoch = 55/100, step = 0/500, loss = 2.2624454498291016\n",
      "epoch = 55/100, step = 100/500, loss = 2.2438879013061523\n",
      "epoch = 55/100, step = 200/500, loss = 2.300168991088867\n",
      "epoch = 55/100, step = 300/500, loss = 2.276522159576416\n",
      "epoch = 55/100, step = 400/500, loss = 2.218456983566284\n",
      "accVal = 13.15%\n",
      "epoch = 56/100, step = 0/500, loss = 2.2645812034606934\n",
      "epoch = 56/100, step = 100/500, loss = 2.236752986907959\n",
      "epoch = 56/100, step = 200/500, loss = 2.1725265979766846\n",
      "epoch = 56/100, step = 300/500, loss = 2.2370169162750244\n",
      "epoch = 56/100, step = 400/500, loss = 2.3263919353485107\n",
      "accVal = 11.58%\n",
      "epoch = 57/100, step = 0/500, loss = 2.2864503860473633\n",
      "epoch = 57/100, step = 100/500, loss = 2.3266522884368896\n",
      "epoch = 57/100, step = 200/500, loss = 2.2444028854370117\n",
      "epoch = 57/100, step = 300/500, loss = 2.2790403366088867\n",
      "epoch = 57/100, step = 400/500, loss = 2.3267812728881836\n",
      "accVal = 13.77%\n",
      "epoch = 58/100, step = 0/500, loss = 2.189596176147461\n",
      "epoch = 58/100, step = 100/500, loss = 2.262404203414917\n",
      "epoch = 58/100, step = 200/500, loss = 2.243987798690796\n",
      "epoch = 58/100, step = 300/500, loss = 2.300260066986084\n",
      "epoch = 58/100, step = 400/500, loss = 2.2777211666107178\n",
      "accVal = 14.21%\n",
      "epoch = 59/100, step = 0/500, loss = 2.296166181564331\n",
      "epoch = 59/100, step = 100/500, loss = 2.3041815757751465\n",
      "epoch = 59/100, step = 200/500, loss = 2.248542547225952\n",
      "epoch = 59/100, step = 300/500, loss = 2.3036606311798096\n",
      "epoch = 59/100, step = 400/500, loss = 2.29823637008667\n",
      "accVal = 13.9%\n",
      "epoch = 60/100, step = 0/500, loss = 2.3190298080444336\n",
      "epoch = 60/100, step = 100/500, loss = 2.2656705379486084\n",
      "epoch = 60/100, step = 200/500, loss = 2.2630460262298584\n",
      "epoch = 60/100, step = 300/500, loss = 2.3021082878112793\n",
      "epoch = 60/100, step = 400/500, loss = 2.30527925491333\n",
      "accVal = 13.73%\n",
      "epoch = 61/100, step = 0/500, loss = 2.3083770275115967\n",
      "epoch = 61/100, step = 100/500, loss = 2.2701210975646973\n",
      "epoch = 61/100, step = 200/500, loss = 2.286003828048706\n",
      "epoch = 61/100, step = 300/500, loss = 2.296649217605591\n",
      "epoch = 61/100, step = 400/500, loss = 2.3084232807159424\n",
      "accVal = 11.07%\n",
      "epoch = 62/100, step = 0/500, loss = 2.2951295375823975\n",
      "epoch = 62/100, step = 100/500, loss = 2.251918077468872\n",
      "epoch = 62/100, step = 200/500, loss = 2.2530481815338135\n",
      "epoch = 62/100, step = 300/500, loss = 2.287367582321167\n",
      "epoch = 62/100, step = 400/500, loss = 2.2794666290283203\n",
      "accVal = 13.93%\n",
      "epoch = 63/100, step = 0/500, loss = 2.233488082885742\n",
      "epoch = 63/100, step = 100/500, loss = 2.268423080444336\n",
      "epoch = 63/100, step = 200/500, loss = 2.3227744102478027\n",
      "epoch = 63/100, step = 300/500, loss = 2.2567145824432373\n",
      "epoch = 63/100, step = 400/500, loss = 2.2813241481781006\n",
      "accVal = 13.46%\n",
      "epoch = 64/100, step = 0/500, loss = 2.240281581878662\n",
      "epoch = 64/100, step = 100/500, loss = 2.2109720706939697\n",
      "epoch = 64/100, step = 200/500, loss = 2.320971965789795\n",
      "epoch = 64/100, step = 300/500, loss = 2.249821662902832\n",
      "epoch = 64/100, step = 400/500, loss = 2.2600834369659424\n",
      "accVal = 12.13%\n",
      "epoch = 65/100, step = 0/500, loss = 2.289111375808716\n",
      "epoch = 65/100, step = 100/500, loss = 2.2738606929779053\n",
      "epoch = 65/100, step = 200/500, loss = 2.316805124282837\n",
      "epoch = 65/100, step = 300/500, loss = 2.24927020072937\n",
      "epoch = 65/100, step = 400/500, loss = 2.2456984519958496\n",
      "accVal = 13.04%\n",
      "epoch = 66/100, step = 0/500, loss = 2.2416634559631348\n",
      "epoch = 66/100, step = 100/500, loss = 2.251400947570801\n",
      "epoch = 66/100, step = 200/500, loss = 2.2282209396362305\n",
      "epoch = 66/100, step = 300/500, loss = 2.259567975997925\n",
      "epoch = 66/100, step = 400/500, loss = 2.2853007316589355\n",
      "accVal = 13.74%\n",
      "epoch = 67/100, step = 0/500, loss = 2.2723305225372314\n",
      "epoch = 67/100, step = 100/500, loss = 2.3002986907958984\n",
      "epoch = 67/100, step = 200/500, loss = 2.2681822776794434\n",
      "epoch = 67/100, step = 300/500, loss = 2.270676612854004\n",
      "epoch = 67/100, step = 400/500, loss = 2.2322580814361572\n",
      "accVal = 14.14%\n",
      "epoch = 68/100, step = 0/500, loss = 2.2620909214019775\n",
      "epoch = 68/100, step = 100/500, loss = 2.270573139190674\n",
      "epoch = 68/100, step = 200/500, loss = 2.2513484954833984\n",
      "epoch = 68/100, step = 300/500, loss = 2.2774455547332764\n",
      "epoch = 68/100, step = 400/500, loss = 2.2867209911346436\n",
      "accVal = 14.89%\n",
      "epoch = 69/100, step = 0/500, loss = 2.2860000133514404\n",
      "epoch = 69/100, step = 100/500, loss = 2.239675283432007\n",
      "epoch = 69/100, step = 200/500, loss = 2.2715420722961426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m cl\u001b[39m.\u001b[39minit_model(bmodel)\n\u001b[1;32m----> 5\u001b[0m bmodel\u001b[39m.\u001b[39;49mtrainModel(training_loader, test_loader, epochs, lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, writer\u001b[39m=\u001b[39;49mwriter, PATH\u001b[39m=\u001b[39;49mPATH)\n",
      "Cell \u001b[1;32mIn [7], line 250\u001b[0m, in \u001b[0;36mCifar10NetworkBin.trainModel\u001b[1;34m(self, training_loader, test_loader, epochs, lr, writer, PATH)\u001b[0m\n\u001b[0;32m    248\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    249\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 250\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    253\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch = \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, step = \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(training_loader)\u001b[39m}\u001b[39;00m\u001b[39m, loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Programmi\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programmi\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programmi\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programmi\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Programmi\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32md:\\Programmi\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    303\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    307\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PATH = \"./models/Cifar10Network_binarized_model\"\n",
    "# writer = SummaryWriter(\"./test_grafici/Cifar10NetworkBinarized\")\n",
    "writer = None\n",
    "cl.init_model(bmodel)\n",
    "bmodel.trainModel(training_loader, test_loader, epochs, lr=0.1, writer=writer, PATH=PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a32be92701276002c3a8928f39e8df46bba964be6a742c4fcb624c4dac10f21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
